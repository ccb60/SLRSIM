#' Calculate Long Term Estimate of SLR
#'
#' Given an series of (typically autocorrelated) observations of sea level or
#' mean sea level, and associated time coordinates (usually Dates), return a
#' named vector containing a summary of a generalized least squares linear trend,
#' optionally scaled to annual values.
#'
#' Sea level data tends to be highly autocorrelated over multiple time
#' scales. This function uses generalized least squares (GLS) to fit a linear
#' model with a correlation structure defined by fitting an AR1 process to model
#' residuals.
#'
#' An AR1 structure is an "autoregressive process of order 1", that is, each
#' observation is regressed against the prior observation.  Informally, the
#' correlations structure is based the lag 1 autocorrelation, so that modeled
#' autocorelations between observations are equal to the lag 1 autocorrelation
#'  raised to the power of the number of timesteps between the two observations.
#'
#' The AR1 structure may not be ideal for all tidal time series. The AR1 process
#' is a simple assumption about correlation structures that reduces the risk of
#' overstating the precision of trend estimates.
#'
#' Slope estimates from the AR1 models tend (in this context) to be
#' indistinguishable from estimates generated by ordinary least squares, but the
#' uncertainty of the estimates is substantially greater.  As observations are
#' not independent, ordinary least squares (which assumes independence) will
#' overstate model precision.
#'
#' The function is generally useful with data averaged over monthly or longer
#' periods.  The function's original use case was based on secondary analysis of
#' NOAA "mean trend" data.  That data contains "monthly mean sea level without
#' the regular seasonal fluctuations due to coastal ocean temperatures,
#' salinities, winds, atmospheric pressures, and ocean currents." Where regular
#' seasonal patterns are evident, explicit modeling is probably called for.
#'
#' The function does not fit any periodic terms. Thus it is is generally
#' not suitable for use with "raw" sea level observations, especially those
#' collected at high resolution, like NOAA hourly data. Tidal data contains many
#' periodic components (~ 24:50 tidal period, spring tide / neap tide cycles,
#' annual cycles, etc.). Those periodic structures will affect estimates of
#' autocorrelation, ultimately making estimates of model uncertainty unreliable.
#' If working from high frequency data, short-term structure should be removed
#' before using this function, typically by calculating average sea level over
#' convenient and meaningful time periods, like months or years.
#'
#' This function mimics the calculation of slopes and associated uncertainties
#' conducted by NOAA. While we have not done a comprehensive review, where we
#' have checked, results of AR1 models agree with with NOAA's trend estimates
#' within rounding error.
#'
#' NOAA calculates slopes based on a modified monthly water level data series.
#' It is not the same as what one gets if one downloads monthly mean water level
#' through the standard NOAA API. The values NOAA uses for this calculation are
#' "monthly mean sea level without the regular seasonal fluctuations due to
#' coastal ocean temperatures, salinities, winds, atmospheric pressures, and
#' ocean currents."  The seasonally detrended data is also available from NOAA
#' via a separate API.  The `prov_meantrend` data included in `SLRSIM` is an
#' example, for Providence, Rhode Island.
#'
#' `SLRSIM` provides a a function, `get_sl_trend_data()` to access these data,
#' but if all you need is the slope and uncertainty estimates, you can get them
#' directly using `get_slr_trend()`
#'
#' @details
#' If `t_fit = FALSE`, the function fits an autocorrelation structure to
#' the sequential observations, ordered by `.dt`, without regard to how much
#' time has passed between subsequent observations. This approach works well
#' with evenly spaced dates and complete or nearly complete records, but is
#' less successful if there are many periods with missing observations.
#'
#' If `t_fit = TRUE`, the function fits an autocorrelation structure based on
#' the time coordinate (.dt). In this case, the time coordinate must be integer
#' valued. POSIXct dates and times are floating point values, while POSIXlt are
#' lists, so neither is suitable, but R's Date class is an integer under the
#' hood. It is easy to build Dates, either by converting POSIX times with
#' `as.Date()` or by building them up from month, day, and year information with
#' `as.Date(paste(year, month, 15, sep = '-')`.
#'
#' The slope estimate and standard error are scaled from days (based on Dates)
#' to (approximate) annual values by multiplying by 365.25.  This means that if
#' you pass a time coordinate that is NOT of class Date, you need to scale the
#' results appropriately.
#'
#' This function is a convenience wrapper around `nlme::gls(..., correlation =
#' corAR1())`. For more control, or to examine model diagnostics, the user
#' should fit the generalized least squares model directly.
#'
#' @param .data Source data frame for data.  Use NULL if no data frame is used
#'         and all data is passed from
#'         the enclosing environment.
#' @param .sl  The data variable (usually found in the data frame) showing sea
#'         level or mean sea level.  Must be a named variable, not an
#'         expression.
#' @param .dt   Data variable containing corresponding midpoint dates for the
#'        period of averaging used to calculate .sl. Must be a named variable,
#'        not an expression. Midpoint dates for a given month can be approximated
#'        with `as.Date(paste(year, month, 15, sep = '-')`
#' @param .ci P value for two sided confidence intervals for the estimated
#'        slope, based on a (possibly naive) normal approximation.
#' @param t_fit Should the model be fit based on the time coordinate, or
#'        only on the sequence of observations in the data?  Setting this
#'        to TRUE is safer if you are uncertain of the sequence of observations
#'        in the source data, or  significant missing values in your data.
#' @param by_year Boolean indicating whether the results should be scaled to
#'        annual values by multiplying by 365.25. If `.dt` is not a Date, this
#'         is ignored, and no scaling is conducted.
#' @return
#' A named vector, with the following components (some may be moved to
#' attributes in the future).
#'  \describe{
#'    \item{Estimate}{The trend. Units depend on source data and value of
#'          the `by_year` argument. If data is passed as Dates and  `by_year`
#'          is TRUE, units are per year, otherwise per unit of the .dt variable.}
#'    \item{Std_Err}{Estimated Standard Error of the trend.  Units as above.}
#'    \item{P_Val}{P value of the trend.  }
#'    \item{Lower_CI}{Lower confidence interval for the }
#'    \item{Upper_CI}{Upper confidence interval for }
#'    \item{CI_P}{Nominal p value for the confidence intervals}
#'    \item{span}{How many calander years are represented in the data?}
#'    \item{start}{First year included in the data}
#'    \item{stop}{last year included in the data}
#'}
#'
#' @export
#'
#' @examples
#'  prov_meantrend$MSL <- prov_meantrend$MSL * 1000 (convert to mm)
#' # Basic Usage
#' slr_slope(prov_meantrend, MSL, MidDate)
#' # if not passed dates, the result is unscaled
#'  slr_slope(prov_meantrend, MSL, unclass(MidDate))
#' # One can also fit the model to the time coordinate explicitly
#' slr_slope(prov_meantrend, MSL, MidDate, t_fit = TRUE)
slr_slope <- function(.data, .sl, .dt,
                      ci = 0.95, t_fit = FALSE, by_year = TRUE) {

  # Ugly argument checks, since they doesn't provide nice error messages.
  stopifnot(is.data.frame(.data) | is.null(.data))
  stopifnot(inherits(t_fit, 'logical'))
  stopifnot(inherits(by_year, 'logical'))

  # We want to be able to accept arguments as unquoted names or quoted names.
  # `ensym()`  captures only names, not expressions
  sl_sym <- rlang::ensym(.sl)
  date_sym<- rlang::ensym(.dt)

  sl <- rlang::eval_tidy(sl_sym, .data)
  the_date <- rlang::eval_tidy(date_sym, .data)

  # If we got dates, and by_year is TRUE, we want to scale to years, otherwise
  # we apply no scaling, anfd leave that to the user.
  if(inherits(the_date, 'Date') && by_year) {
    multiplier <- 365.25
  }
  else {
    multiplier <- 1
  }

  # Reorder the data by the time stamp. Only essential if t_fit is FALSE
  sl <- sl[order(the_date)]
  the_date <- the_date[order(the_date)]

  year = as.numeric(format(the_data, format = '%Y'))
  yearspan = max(year) - min(year) + 1

  q_val = - qnorm(.ci/2)

  # We use generalized least squares so we can account for
  # autocorrelation in estimating the error of the slope.
  # The simpler model is for a complete or nearly complete time series. The
  # model based on the time coordinates is slower, and sometimes
  # crashes on large data sets.
  if (t_fit) {
    the_gls <- nlme::gls(sl ~ the_date,
                         correlation = nlme::corAR1(0.75, form = ~the_date))
  }
  else {
    the_gls <- nlme::gls(sl ~ the_date,
                         correlation = nlme::corAR1())
  }

  mod_sum <- as.data.frame(summary(the_gls)$tTable)
  EST <- ccs$Value[2] * 365.25  # convert to year
  SE <- ccs$Std.Error[2]   * 365.25
  P <-  ccs$`p-value`[2]
  low_CI <- est - q_val*SE
  high_CI<- est + q_val*SE
  return(c(Estimate = EST, Std_Err = SE, P_Val = P,
           Lower_CI =  low_CI, Upper_CI = high_CI, CI_P = .ci,
           span = yearsspan, start = min(year), end = max(year)))
}
