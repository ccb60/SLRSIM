% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/slr_slope.R
\name{slr_slope}
\alias{slr_slope}
\title{Calculate Long Term Estimate of SLR}
\usage{
slr_slope(.data, .sl, .dt, ci = 0.95, t_fit = FALSE, by_year = TRUE)
}
\arguments{
\item{.data}{Source data frame for data.  Use NULL if no data frame is used
and all data is passed from
the enclosing environment.}

\item{.sl}{The data variable (usually found in the data frame) showing sea
level or mean sea level.  Must be a named variable, not an
expression.}

\item{.dt}{Data variable containing corresponding midpoint dates for the
period of averaging used to calculate .sl. Must be a named variable,
not an expression. Midpoint dates for a given month can be approximated
with \verb{as.Date(paste(year, month, 15, sep = '-')}}

\item{t_fit}{Should the model be fit based on the time coordinate, or
only on the sequence of observations in the data?  Setting this
to TRUE is safer if you are uncertain of the sequence of observations
in the source data, or  significant missing values in your data.}

\item{by_year}{Boolean indicating whether the results should be scaled to
annual values by multiplying by 365.25. If \code{.dt} is not a Date, this
is ignored, and no scaling is conducted.}

\item{.ci}{P value for two sided confidence intervals for the estimated
slope, based on a (possibly naive) normal approximation.}
}
\value{
A named vector, with the following components (some may be moved to
attributes in the future).
\describe{
\item{Estimate}{The trend. Units depend on source data and value of
the \code{by_year} argument. If data is passed as Dates and  \code{by_year}
is TRUE, units are per year, otherwise per unit of the .dt variable.}
\item{Std_Err}{Estimated Standard Error of the trend.  Units as above.}
\item{P_Val}{P value of the trend.  }
\item{Lower_CI}{Lower confidence interval for the }
\item{Upper_CI}{Upper confidence interval for }
\item{CI_P}{Nominal p value for the confidence intervals}
\item{span}{How many calander years are represented in the data?}
\item{start}{First year included in the data}
\item{stop}{last year included in the data}
}
}
\description{
Given an series of (typically autocorrelated) observations of sea level or
mean sea level, and associated time coordinates (usually Dates), return a
named vector containing a summary of a generalized least squares linear trend,
optionally scaled to annual values.
}
\details{
Sea level data tends to be highly autocorrelated over multiple time
scales. This function uses generalized least squares (GLS) to fit a linear
model with a correlation structure defined by fitting an AR1 process to model
residuals.

An AR1 structure is an "autoregressive process of order 1", that is, each
observation is regressed against the prior observation.  Informally, the
correlations structure is based the lag 1 autocorrelation, so that modeled
autocorelations between observations are equal to the lag 1 autocorrelation
raised to the power of the number of timesteps between the two observations.

The AR1 structure may not be ideal for all tidal time series. The AR1 process
is a simple assumption about correlation structures that reduces the risk of
overstating the precision of trend estimates.

Slope estimates from the AR1 models tend (in this context) to be
indistinguishable from estimates generated by ordinary least squares, but the
uncertainty of the estimates is substantially greater.  As observations are
not independent, ordinary least squares (which assumes independence) will
overstate model precision.

The function is generally useful with data averaged over monthly or longer
periods.  The function's original use case was based on secondary analysis of
NOAA "mean trend" data.  That data contains "monthly mean sea level without
the regular seasonal fluctuations due to coastal ocean temperatures,
salinities, winds, atmospheric pressures, and ocean currents." Where regular
seasonal patterns are evident, explicit modeling is probably called for.

The function does not fit any periodic terms. Thus it is is generally
not suitable for use with "raw" sea level observations, especially those
collected at high resolution, like NOAA hourly data. Tidal data contains many
periodic components (~ 24:50 tidal period, spring tide / neap tide cycles,
annual cycles, etc.). Those periodic structures will affect estimates of
autocorrelation, ultimately making estimates of model uncertainty unreliable.
If working from high frequency data, short-term structure should be removed
before using this function, typically by calculating average sea level over
convenient and meaningful time periods, like months or years.

This function mimics the calculation of slopes and associated uncertainties
conducted by NOAA. While we have not done a comprehensive review, where we
have checked, results of AR1 models agree with with NOAA's trend estimates
within rounding error.

NOAA calculates slopes based on a modified monthly water level data series.
It is not the same as what one gets if one downloads monthly mean water level
through the standard NOAA API. The values NOAA uses for this calculation are
"monthly mean sea level without the regular seasonal fluctuations due to
coastal ocean temperatures, salinities, winds, atmospheric pressures, and
ocean currents."  The seasonally detrended data is also available from NOAA
via a separate API.  The \code{prov_meantrend} data included in \code{SLRSIM} is an
example, for Providence, Rhode Island.

\code{SLRSIM} provides a a function, \code{get_sl_trend_data()} to access these data,
but if all you need is the slope and uncertainty estimates, you can get them
directly using \code{get_slr_trend()}

If \code{t_fit = FALSE}, the function fits an autocorrelation structure to
the sequential observations, ordered by \code{.dt}, without regard to how much
time has passed between subsequent observations. This approach works well
with evenly spaced dates and complete or nearly complete records, but is
less successful if there are many periods with missing observations.

If \code{t_fit = TRUE}, the function fits an autocorrelation structure based on
the time coordinate (.dt). In this case, the time coordinate must be integer
valued. POSIXct dates and times are floating point values, while POSIXlt are
lists, so neither is suitable, but R's Date class is an integer under the
hood. It is easy to build Dates, either by converting POSIX times with
\code{as.Date()} or by building them up from month, day, and year information with
\verb{as.Date(paste(year, month, 15, sep = '-')}.

The slope estimate and standard error are scaled from days (based on Dates)
to (approximate) annual values by multiplying by 365.25.  This means that if
you pass a time coordinate that is NOT of class Date, you need to scale the
results appropriately.

This function is a convenience wrapper around \code{nlme::gls(..., correlation = corAR1())}. For more control, or to examine model diagnostics, the user
should fit the generalized least squares model directly.
}
\examples{
 prov_meantrend$MSL <- prov_meantrend$MSL * 1000 (convert to mm)
# Basic Usage
slr_slope(prov_meantrend, MSL, MidDate)
# if not passed dates, the result is unscaled
 slr_slope(prov_meantrend, MSL, unclass(MidDate))
# One can also fit the model to the time coordinate explicitly
slr_slope(prov_meantrend, MSL, MidDate, t_fit = TRUE)
}
