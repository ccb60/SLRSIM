---
title: "NOAA Data Access Functions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Access Functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = 'center',
  fig.width = 5, fig.height = 4
)
```

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />


```{r}
library(tidyverse)
library(httr)      # an R equivalent to python's requests library
library(SLRSIM)
```


# Introduction
`SRLSIM` provides two main functions to access the NOAA data API and retrieve
water level data or predicted water levels.

`call_api()` is a simple wrapper around the NOAA data API, allowing you to call
the API from R, and retrieve water level data or predictions data as a familiar 
data frame.  Because of limitation of the NOAA API, you can request no more than
one year's worth of data in each request.

`retrieve_data()` encapsulates the logic to send multiple calls to the API and
assemble a larger (multi-year) data frame.  

Both functions ONLY retrieve hourly data.  That design choice 
reflects the purpose of SLRSIM to analyze long-term SLR records. NOAA's six
minute data products are substantially less convenient to work with and, in our
context, provide little benefit.

1.  The API restricts six-minute downloads to monthly intervals, which 
    complicates coding for downloading long data records, since you have to keep
    track of the length of each month, adjust for leap years, etc.
2.  Data collected every six minutes is even noisier and more highly 
    autocorrelated than hourly data, complicating statistical analyses.  
3.  Long-term data based on six minute intervals quickly generates huge data
    structures.  These are often large enough to pose challenges for common R
    workflows. And  
3.  Older data is often only available at hourly intervals, so for SLR analysis
    we'll often end up working with hourly data anyway.

## Function `call_api()`
This function submits a single request to the API, and is the lower-level
workhorse function. The expectation is that users will usually call the higher
level functions. Note that this is designed for hourly data downloaded on a calender day basis.

Dates can be submitted as objects of class `Date`, or as "YYYYmmdd" or
"mm/dd/YYYY" formatted strings. Error checking for properly formatted strings
is minimal.  Again, since this is intended as  a low-level function, that's your
responsibility.

```{r} 
.station    <- '8454000'  # Providence, RI
.begin_date <- '20200801'
.end_date   <- '20200805'
```

### Observed Data
You specify what kind of data you want via the fourth function argument, 
`.which`.  `.Which` can be either "observed" or "predicted".
```{r}
head(call_api(.station, .begin_date, .end_date, 'observed'), 20)
```
### Tide predictions
Note that the name of the second data vector is "water_level" regardless of 
which type of data you requested.  As this is intended to be a low-level access function, this should not ordinarily be a problem.
```{r}
a <- call_api(.station, .begin_date, .end_date, 'predicted')
head(a,20)
```
A certain amount of metadata in encoded in attributes.
```{r}
names(attributes(a))
```

So, if you really are no longer sure whether that data frame contained 
observations or predictions, you can check
```{r}
attr(a, 'which')
```

### Other Parameters
Other function parameters allow you to specify:
1.  A tidal datum. (`.datum`)  
2.  Metric versus english units. (`.units`)
3.  A "Time Format"   (`.timefmt`) which can take on exactly three values --
    'gmt', 'lst', or 'lst_ldt, for  "Greenwich Mean Time", local standard time, 
    or (local) clock time, respectively.
4.  A formal timezone. (`.tz`)

#### Timezones are messy
In practice, local clock time can be confusing, because the UTC offset shifts
twice a year to address summer "daylight savings Time". This leads to confusing results like dates with more than or fewer than 24 hourly observations.  Thus
for our purposes 'lst_ldt' is generally not recommended.

The times returned by the NOAA API are returned as strings. The `call_api()`
function converts the strings  to `POSIXct` objects. To handle the conversion
correctly, the function needs to know the timezone so that a particular string
can be unambiguously tied to a specific UTC time.

It takes TWO parameters to handle this conversion consistently, `.timefmt` and
`.tz`.  `.timefmt` controls the times returned by the NOAA API, and `.tz`
controls conversion from the string representation to a POSIXct object.  

POSIXct objects are represented internally as an integer (number of seconds
since the time origin) and a `tzone` attribute.  The `print()` function for
POSIXct objects (and many other functions on POSIXct) interprets the integer in light of the timezone, and generates a string represntation.  That means the 
same underlying TIME (integer number of seconds) can be represented by many different strings, depending on the value of the `tzone` attribute.  Or conversely, the same string can be interpreted as multiple different times, based on `.tz`.  So if you need the internal representation to be right, you need to specify a correct timezone.

The defaults set `.timefmt = 'gmt'` and `.tz = 'UTC'`. For many purposes, this
is a good choice, as it correctly records time in UTC, and creates a consistent
internal representation. However, there are times when having times -- and dates
-- in local time can be important.

If you downloaded data with `.timefmt = 'lst'`,  or `.timefmt = 'lst_ldt'`, the
internal representation of times will be off by the (possibly changing) timezone
offset.

Unless you are going to coordinate these times with times from other
analyses, the internal representation may not matter.

See the help page for these functions and `?timezones` for more insight. 

TODO:  Add error check to see if .tz in not missing if .timefmt is not 'gmt'.

Here's what happens if you chose not to specify a timezone when you specify
`.timefmt = 'lst'`.

```{r}
a <- call_api(.station, .begin_date, .end_date, 'observed',
              .timefmt = 'gmt')
b <- call_api(.station, .begin_date, .end_date, 'observed',
              .timefmt = 'lst')
```

The function requests data by the day.  The API returns data starting at 
midnight.  The request based on `.timefmt = 'gmt'` starts at midnight UTC.
The request based on `.timefmt = 'lst'` starts at midnight Eastern Standard 
Time.  Thus the data is identical, but offset by the five hour timezone offset.

```{r}
all.equal(a[6:106, 'water_level'], b[1:101, 'water_level'])
```

Unfortunately, the function default to interpreting times as UTC, unless you specify something else with `.tz`.  It is impossible to convert a time to
POSIXct without a timezone. If you do not specify a timezone, (e.g., by 
passing `.tz = ''`) the function inherits the behavior of `as.POSIXct()`, and
guesses the local time zone (from your computer's operating system).

The times reported for `.timefmt = 'gmt'` are correct.
```{r}
a[6:15, 'datetime']
```

But the times reported for `.timefmt = 'lst'` are confused. The strings are
right, but the allocation to UTC is wrong, so the internal representations, are
incorrect.
```{r}
b[1:10, 'datetime']
```

You can correct the internal representation by specifying a timezone.
```{r}
c <- call_api(.station, .begin_date, .end_date, 'observed',
              .timefmt = 'lst', .tz = 'Etc/GMT+5')
c[1:8, 'datetime']
```
Note that the timezone shows the correct offset from UTC.

We can also check that by looking at the 'tzone' attribute of datetime.
```{r}
attr(a$datetime[[1]], 'tzone')
attr(c$datetime[[1]], 'tzone')
```

For purposes of looking at changing sea level over a period of decades, that
five hour offset would not matter (much). Further, if you are not comparing
these times to other times (say of weather records), only the string
representation is going to matter.  Still it is worth knowing how to handle this
correctly.

## Function `retrieve_data()`
This function marshals multiple API calls to assemble multi-year data sets.

### Observations
As with the `call_api()` function, you need the tell `retrieve_dat()` .which
kind of data you want. 
```{r}
.station    <- '8454000'  # Providence, RI
a <- retrieve_data(.station, 2000, 2002, .which = 'observed')
a
```

### Predictions
```{r}
b <- retrieve_data(.station, 2000, 2002, .which = 'predicted')
b
```

Again, the attributes contain relevant metadata.
```{r}
attr(a, 'which')
attr(b, 'which')
attr(b, 'station')
attr(b, 'timefmt')
```

### Memory Issues
We retrieve hourly observations, so there are 365 * 24 = 8,760 observations each
year, with two double values (time and water level) per observation.  Even
ignoring attributes, this means each year's data consumes roughly

$$8,760 \text{ hours per year} \times 
2 \text{ values per hour} \times 
8 \text{ bytes per value} = 
140,160 \text{ bytes per year}$$

or 14 kilobytes. 100 years of data will require 1.4 megabytes. That is not large
if stored on disk, but it's probably not a good idea to load several of these
into active memory simultaneously.  It is likely to make R slow to a crawl.

# Function `build_deviations()`
This function pulls both observed and predicted water levels from the NOAA API,
calculates their difference, and discards the (now redundant) predicted values. 
These deviations are used to support modeling of future flood risk, and the 
function is called by the `floodcast_arima()` function. Most users will never 
need this function, but for those interested in exploring simulation
strategies for modeling future food risk, it may come in handy.
```{r}
.begin_yr <- 2019
.end_yr   <- 2020
.station    <- '8454000'

res <- build_deviations(.station,
                        .begin_yr,
                        .end_yr)
```

```{r}
res
```

