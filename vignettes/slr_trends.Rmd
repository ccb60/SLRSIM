---
title: "Assessing Trends in Sea Level"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assessing Trends in Sea Level}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />


```{r setup}
library(tidyverse)
library(nlme)
library(SLRSIM)
```

# Access Data
Our primary source data is based on NOAA's analysis of sea level trends.  The
description on the source web site
(https://tidesandcurrents.noaa.gov/sltrends/sltrends_station.shtml?id=8454000)
says the following, so this is apparently NOT raw data.

> "The plot shows the monthly mean sea level without the regular seasonal
fluctuations due to coastal ocean temperatures, salinities, winds,
atmospheric pressures, and ocean currents. ... The plotted values are relative
to the most recent Mean Sea Level datum established by CO-OPS."

```{r load_data}
prov_meantrend  <- prov_meantrend %>%
  mutate(before_gap = MidDate < as.Date('1955-01-01'))
```

# Looking up NOAA' Long Term Linear Trend Estimate
We first demonstrate use a linear model analysis to calculate the linear trend 
as reported by NOAA on the source web page at 
https://tidesandcurrents.noaa.gov/sltrends/sltrends_station.shtml?id=8454000.

At that site, NOAA reports NOAA reports a value of 2.40 +/- 0.23 mm/year.  While
the meaning of the range given is not defined, it appears to be a 95% confidence 
interval.

We can extract NOAA's estimate and its standard error with `get_sl_trend()`.
Note that these values are rounded.
```{r}
providence_id = 8454000
sl_t <- get_sl_trend(providence_id)
sl_t$trend
sl_t$trendError
```
NOAA reports a value of 2.40 +/- 0.23 mm/year.

#  A Generalized Linear Model Analysis
```{r slr_gls}
the_gls <- gls(MSL_mm ~ MidDate, data=prov_meantrend,
               correlation = corAR1(), method = 'ML')
(s <- summary(the_gls))
```
We need to convert units from days to years.  We can find the info we need
either in the tTable component of the model sumamry:
```{r}
s$tTable[2,1] * 365.2422 
s$tTable[2,2] * 365.2422
```
Or directly 
```{r}
coef(the_gls)[2] * 365.2422
sqrt(vcov(the_gls)[2,2]) * 365.2422
```
So, we get 2.41 +/- 0.12.  If NOAA rounds the estimate only to tenths of 
millimeters, our results match.  Otherwise, they are ever so slightly different.

# The `slr_slope()` Function
The SLR_slope function is a convenience function that wraps up the linear model 
just presented and spits out model coefficients and other metadata without 
requiring you to build your own model.

{TODO:  Add code to make this print out in a nicer way, whether by rounding or 
by creating an s3 class and overriding the print method.}
{TODO:  Evaluate packaging ofoutput for consistency with other fxns}

```{r}
slope_info <- slr_slope(prov_meantrend, MSL_mm, MidDate)
print(round(slope_info[1:5],4))
```

Some metadata is provided in attributes:
```{r}
attributes(slope_info)
```



Setting t_fit = TRUE makes the model run much more slowly, as it is fitting an
autoregressive model based on the time coordinate, rather than just the sequence
of observations.  The resulting fit is (here) not quite identical, but very 
similar.  

`t_fit == TRUE` will handle unevenly spaced observations appropriately, and may 
be preferred if there are many missing values in the source data.  The following code takes about 30 seconds to a minute to run.

```{r cache = TRUE}
system.time({
slope_info <- slr_slope(prov_meantrend, MSL_mm, MidDate, t_fit = TRUE)
print(round(slope_info[1:5],4))
})
```

## Date Time Formats
With `.mode = 'year'`, which is the default, the time coordinate, `.dt`, must 
be either class "Date" or "POSIXct". The last model was run on dates stores as R `Date` objects.  Here we demonstrate a model run on "POSIXct" times.  It
produces results identical to the analysis based on Date objects.
```{r error = TRUE}
tmp <- prov_meantrend %>%
  mutate(MidDate = as.POSIXct(MidDate))

# this works:
slope_info_raw_POSIX <- slr_slope(tmp, MSL_mm, MidDate, t_fit = FALSE)
round(slope_info_raw_POSIX[1:5],4)
```

But with "POSIXt" objects, trying to fit the model with a covariance structure
based on the time coordinate (in seconds) fails.  This is probably because of
the way the `corAR1()` function is implemented internally.
```{r error = TRUE}
slope_info_raw_POSIX <- slr_slope(tmp, MSL_mm, MidDate, t_fit = TRUE)
```

##  The `.mode` Argument
If you set `.mode = 'unscaled', the function will report the trend without
rescaling to annual rates.  The rate and its standard error will be reported in
the same units as used to express `.dt`.

You could, for example, create a new variable that starts at zero in January of 
1900, and increments by one each month. Using `.mode = 'unscaled'` produces an
estimate of the long term average monthly rate of change in sea level.  Scaling
that result to an annual basis by hand shows that the result is equivalent
(although not quite identical) to the result when working with Dates.

```{r}
tmp <- prov_meantrend %>%
  mutate(months = (Year - 1900) * 12 + (Month -1))

(res <- round(slr_slope(tmp, MSL_mm, months, .mode = 'unscaled')[1:5],4))
res[['Estimate']] * 12
```

Setting `t_fit = TRUE` works for unscaled analyses too, and in this case, it
runs somewhat faster, presumably because  `months` has one possible value
per observation (absent missing values), making the process fo constructing
the correlation structure faster.

```{r cache = TRUE}
tmp <- prov_meantrend %>%
  mutate(months = (Year - 1900) * 12 + (Month -1))

system.time(res <- round(slr_slope(tmp, MSL_mm, months, .mode = 'unscaled', 
                        t_fit = TRUE)[1:5],4))
res
res[['Estimate']] * 12
```

## Comparison to Results with Data not Seasonally Corrected
NOAA also releases "raw" monthly mean water level data.  Unlike the seasonally
adjusted values used by NOAA to generate sea level rise rate estimates, these 
"raw" water level data are available through the NOAA API.

If we run the same analysis on the "raw" data, we get similar, but again not
identical, results. The most obvious change in  an increase in the estimated
standard error.  That makes sense, as the "seasonally adjusted" data in effect
removes one source of variability from the data. As that variability is still
present, standard errors are a bit higher.
```{r}
slope_info_raw <- slr_slope(prov_monthly, MSL_mm, MidDate, t_fit = FALSE)
print(round(slope_info_raw[1:5],4))
```

# Has Sea Level Rise Increased Recently?
A frequent question raised in analysis of sea level records is whether the rate
of sea level rise is increasing, as predicted by numerous climate models over
the years. This is a simple question that is rather more complex to answer
statistically than it at first appears.

The simplest approach we take to checking this idea is to fit a piece-wise
linear model to historic water level rise data.  The user can specify a 
"breakpoint", "cutpoint" or "knot" by giving a parameter that specifies what 
portion of the data is to be considered the "recent" portion of the data.

This analysis is embodied in the function `slr_change()`.

The piecewise linear model fits the data with two linear segments that join at a
knot.  The easiest way to specify a knot is by the number of years in the
"recent" period.

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, .span = 20, .mode = 'year')
round(s$summary,4)
round(s$details,4)
s$settings

```

## Alternative Time Coordinates
The location of the knot can also be specified by a time interval (of class
"difftime") by specifying `.mode = 'time'`.  It is inconvenient that the 
"difftime" class does not accept time expressed in months, which is often
going to be the units of interest.  For longer time records (over a few years),
one can calculate the (approximate) number of days based on a number of
intervening months by multiplying the number of months by the average number of 
days in a month, 
$$ Y \text{ Days} \approx X \text{ Months} \times 
\frac{365.2422 \text{ Days per Year}}{12 \text{ Months per Year}}$$


We demonstrate by specifying time in months -- here 240 month , which is 
equivalent to  20 years, as before.  We do not expect results to be identical, 
as the "recent" 240 months will line up with the last observation in the data,
not with calendar years.

```{r}
max(prov_meantrend$MidDate)
(our_span <- round(as.difftime(240 * (365.2422/12), units = 'days')))
```

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, 
                .span = our_span, .mode = 'time')
round(s$summary,4)
round(s$details,4)
s$settings

```

Note that the slopes presented here are untransformed, so expressed in units
of days, and need to be converted to units of interest.

Last, the recent period can be defiend by the number of observations in the
recent period by passing an integer and specifying `.mode = 'count'`.

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, 
                .span = 240, .mode = 'count')
round(s$summary,4)
round(s$details,4)
s$settings

```
Note that the slopes presented here are untransformed, so expressed in units
of days, and need to be converted to units of interest.


## Examining Model Results
It is possible, and sometimes helpful, to retain the model results for further
examination or processing. Because R model objects encapsulate the source data, 
they are often quite large, so `we`slr_change()` does not return the fitted 
model object unless specifically requested.

It is often worth examining the models to make sure you understand how the model
was fit, review model diagnostics, etc.

One circumstance in which the model object is useful is if you want to prepare
a graphic showing the piecewise linear model.

First, we re-run the model indicating that we want to fitted model object.
We then and extract the model component, and draw a graphic.
```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, .span = 20, 
            .mode = 'year', retain_model = TRUE)
```
```{r}
piecewise_gls <- s$model
```

```{r}
summary(piecewise_gls)
```

Notice that the model parameters are untransformed, and thus expressed in 
units per day, not per year.  hi sis likely to be especially disconcerting if 
originl data were a POSIXct object, and thus expressed in seconds. Under those 
conditions, and with the default rounding for the display of model parameters,
it is likely the slopes will appear to be exactly zero.

{TODO: Consider changing code so that rescaling occurs before model fitting
rather than after model fitting, to avoid this problem.}

## Visualizing the Models
```{r  visualize_models, plot_slr_models, fig.height = 7, fig.width = 5}
ggplot(prov_meantrend, aes(MidDate, MSL_mm, group = before_gap)) +
  geom_line(color='grey20', alpha = 0.25 ) +
  geom_line(aes(x = MidDate, y = predict(the_gls)),
            color = 'black', lwd = 1) +
  geom_line(aes(x = MidDate, y = predict(piecewise_gls)),
            color = 'green4', lwd = 1) +
  xlab('Date') + 
  ylab('Monthly Mean Tide Level (m, MSL)')
```

## Researcher Degrees of Freedom
While it's handy to have a function that conducts this analysis, one should not
take the stated level of statistical significance at face value.

This analysis suffers both from risk of motivated reasoning and from a high 
number of "Researcher Degrees of Freedom".

> Wicherts, Jelte M.; Veldkamp, Coosje L. S.; Augusteijn, Hilde E. M.; Bakker, Marjan; van Aert, Robbie C. M.; van Assen, Marcel A. L. M. (2016). Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking. Frontiers in Psychology. 7: 1832. doi:10.3389/fpsyg.2016.01832.

Interest in looking at recent rates of sea level arise from the belief,
supported by numerous models, that anthropogenic climate change should produce
increasing rates of sea level rise during the 21st century. Often, analysis of
SLR during a recent period of time is motivated -- consciously, 
unconsciously, or institutionally -- by the desire to confirm or refute the idea
that a particular data set supports the hypothesis that rates of sea level rise
are increasing.

Any time series with high autocorrelation, produces a quasi-periodic pattern. If
one time period has high values, it is likely the next will as well.  Thus in
the presence of autocorrelation, we anticipate extended runs of above average
values interspersed with runs of below average values.  This pattern can arise
even in the absence of periodic drivers, and can be quite marked for data sets generated by certain ARIMA processes.

That implies that simply by chance, some periods of time will have above average
slopes, while others will have below average slopes. The most recent period may,
again by chance, have unusually high rate of sea level rise, and attract the
attention of an investigator.

The investigator also has several "tweaks" at their disposal that can affect the
outcome of the analysis. Of crucial importance for periodic or quasi-periodic
data sets, The investigator can select the data being analyzed (location of
tidal station, starting and ending dates) and the length of the recent period 
being analyzed (`.span`). An evil investigator bent on propoganda or obfuscation
has significant control over the outcome of their analysis, in ways largely
opaque to readers.  By the same token, an honest analyst can readily draw
conclusions not strongly supported by the data, by ignoring the way apparently
small  choices can alter the results of the analysis.

There is no mathematical method to identify or account for all possible 
researcher degrees of freedom and thus provide automated or quantitative
protection against sloppiness or biased analyses.  In this context, if results
are sensitive to starting and ending dates, exact value of .span`, or other 
details of the analysis, the results should be treated with suspicion.

# Is The Most Recent SLR Rate Exceptional?
We can get a pretty good idea of whether recent rates of sea level 
rise are higher than expected based on the historic record.  After all,
we could look at the estimated rates of sea level rise over all or many prior 
periods of time and compare them to recent periods of interest.

Another property of periodic or quasi-periodic data is that short term estimates
of (linear) slopes (Where "short term" means `.span` is less than half the
period of an underlying periodic pattern) will be much more variable than
long-term slopes (where "long term implies a slope estimated over more than one
period).  Note that this applies both the true periodic data and to
quasi-periodic data generated through an ARIMA process.

By chance, some short-term slopes will be "significantly" different from the long-term slopes, and yet really only reflect the fact that the short-term slope 
was calculated on the "rising arm" of a periodic or quasi-periodic underlying pattern.

Although the "recent" period of time may have a slope that is statistically 
higher than the mean slope in the previous period of record, that may partly be 
because we are comparing a short period to a long period.

Perhaps other similar-length periods in the historic record have similar high slopes. Before we claim that recent rates of sea level rise are unusually high, 
we should place recent rates in historical context.

The `all_slr_change()` function conducts an analysis based on that 
idea.  It calculates rates of change in sea level for a subset of prior possible time periods, and compares the rate of change in sea level of the most recent 
period to all prior similar time periods.

We first look at twenty-year long periods of time (sometimes somewhat less due 
to missing values, as shown here), starting and ending at the beginning of each
calendar year.

We first look at the total number of rates of SLR we estimated, and how many were 
equal to or higher than the most recent period.
```{r}
all_slopes <- all_slr_change(prov_meantrend, MSL_mm, MidDate, .span = 20,
               .interval = 1, .mode = 'year')
all_slopes[1:2]
```
Only one of the forty nine prior twenty year (or less) periods showed a rate of sea level rise higher than the most recent period.

Not all of the slopes are based on 20 year records, because of missing data. We should look at the details.

```{r}
all_slopes$df
```

Notice that the first several records (1953 through 1956) span missing data, 
and so all start on the same date (1956-09-15), but have different ending dates.

The default behavior of `all_slr_change()` will report a slope for any period 
with at least 75% of the maximum number of data points used to calculate any of
the slopes.  That behavior can be altered with the `.threshold` argument. 

Similarly, the final regression slope (labeled 2002) is based on partial data in
the Year 2021 (we only have January and February data). It thus relies on almost exactly the same data as the second to last slope, labeled as 2001.


{TODO:  Add code to filter out records that lack data for whole years, to avoid 
this, or add warnings or a flag to the df.}

We can get a more visceral sense of how unusual the recent period of high sea
level rise rates are by plotting them year by year.
```{r}
ggplot(all_slopes$df, aes(x = label, y = slopes)) +
  geom_line() +
  geom_point(aes(color = pvals < 0.05)) +
  xlab('Starting Year') +
  ylab('20 Year Rate of  SLR (mm per year)')
```
Since about 1977, rates of SLR have generally been close to 3.5 to 4.5 mm per
year, with a quasi-periodic pattern. While SLR rates over the last few years 
are the highest on record, they are not wildly out of line with prior periods 
of especially high SLR.

##  Controlling the Length Over Which to Estimate Slopes
Perhaps those resutls are affected by the `.span` used.  You can look at shorter 
or longer time periods  by setting the`.span` argument.
```{r}
all_slopes <- all_slr_change(prov_meantrend, MSL_mm, MidDate, .span = 15,
               .interval = 1, .mode = 'year')

ggplot(all_slopes$df, aes(x = label, y = slopes)) +
  geom_line() +
  geom_point(aes(color = pvals < 0.05)) +
  xlab('Starting Year') +
  ylab('15 Year Rate of  SLR (mm per year)')
```
Over shorter periods of time, slopes vary more, and are less likely to be
statistically significant. Notice that if you look at a 15 year `.span`,
as here, recent slopes are not especially unusual.

This points out how easy it is to be mislead by looking at one or two short-term slopes without putting them in longer-term context, and evaluating slightly
different time periods.

## Controlling the Frequency of Estimates
Successive estimates are based on nearly the same data, and so their estimates 
will be similar.  While it is possible to model that process explicitly, it is 
also reasonable to just look at slopes that do not overlap so thoroughly, by
skipping years.

You can request that modeled interval start less often by passing a value to the `.interval` argument.  With `.mode = 'year'`, as here, `.interval` must be an integer.  If you want more frequent overlapping samples, you must use `.mode = 'duration'` and pass time intervals an object of class "difftime".

A warning: the results can be a bit confusing, since the years that start each
interval start with the first year in the data, regardless of how that year
relates to the length of yor `.span`.  For example, if yo uare using `.span =
10`, you might want to look at years that start with zero or five. The function
offers no way to control the starting date directly, so you need to do reshaping
the source data.

We use the `magrittr` pipe, `%>%`,to modify the source data and make
the starting dates for each slope line up with years ending in fives and zeros.
The `all_slr_change()` function's first argument is a data frame, facilitating 
this kind of pre-processing.
```{r}
all_slopes <- prov_meantrend %>%
  filter(Year >= 1940) %>% 
  all_slr_change(MSL_mm, MidDate, .span = 10,
                 .interval = 5, .mode = 'year')
all_slopes$df
```

Recall that if there is some missing data, one or more slopes may be fitted to a partial record that starts in an "off" year. The slope labeled "1955" here 
actually spans from 1956 through 1964.

```{r}
ggplot(all_slopes$df, aes(x = label, y = slopes)) +
  geom_line() +
  geom_point(aes(color = pvals < 0.05)) +
  xlab('Starting Year') +
  ylab('10 Year Rate of  SLR (mm per year)')
```

We have far fewer slopes, but the recent ones don't look especially 
exceptional.


## Other Time Intervals
Using  `.mode = 'duration'` allows you to specify `.span` and `.interval` as
`difftime` objects, which can sometimes be convenient.  R's `difftime` objects
are numbers under the hood, coupled to an attribute specifying time units. The
longest time units available are weeks.  We recommend working with days.
Conversion to years and months can be based on the average length of a year, at
365.2422 days.

Here we specify a roughly ten year `interval`.span` as 3650 days, and ask for a 
new slope based on data windows spaced 180 days days apart.  Results are
qualitatively similar to `.mode = 'year'` and `.span = 10`, as one might expect.
Here we can see more clearly when periods of high SLR began and ended.

```{r}
all_slopes <- all_slr_change(prov_meantrend, MSL_mm, MidDate, 
                              .span = as.difftime(3650, units = 'days'),
               .interval = as.difftime(180, units = 'days'), 
               .mode = 'duration')

ggplot(all_slopes$df, aes(x = label, y = slopes)) +
  geom_line() +
  geom_point(aes(color = pvals < 0.05)) +
  xlab('Starting Year') +
  ylab('520 Week Rate of  SLR (mm per day)')
```

## By Number of Samples
If you use `.mode = 'count'`, you can specify things int erms of the 
NUMBER of observations.
```{r error = TRUE}
(all_slopes <- all_slr_change(prov_meantrend, MSL_mm, MidDate, 
                              .span = 100,
                              .interval = 15, 
                              .mode = 'count'))
```
