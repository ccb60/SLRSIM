---
title: "Assessing Trends in Sea Level"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assessing Trends in Sea Level}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />


```{r setup}
library(tidyverse)
library(nlme)
library(SLRSIM)
```

# Access Data
Our primary source data is based on NOAA's analysis of sea level trends.  The
description on the source web site
(https://tidesandcurrents.noaa.gov/sltrends/sltrends_station.shtml?id=8454000)
says the following, so this is apparently NOT raw data.

> "The plot shows the monthly mean sea level without the regular seasonal
fluctuations due to coastal ocean temperatures, salinities, winds,
atmospheric pressures, and ocean currents. ... The plotted values are relative
to the most recent Mean Sea Level datum established by CO-OPS."

```{r load_data}
prov_meantrend  <- prov_meantrend %>%
  mutate(before_gap = MidDate < as.Date('1955-01-01'))
```

# Looking up NOAA' Long Term Linear Trend Estimate
We first demonstrate use a linear model analysis to calculate the linear trend 
as reported by NOAA on the source web page at 
https://tidesandcurrents.noaa.gov/sltrends/sltrends_station.shtml?id=8454000.

At that site, NOAA reports NOAA reports a value of 2.40 +/- 0.23 mm/year.  While
the meaning of the range given is not defined, it appears to be a 95% confidence 
interval.

We can extract NOAA's estimate and its standard error with `get_sl_trend()`.
Note that these values are rounded.
```{r}
providence_id = 8454000
sl_t <- get_sl_trend(providence_id)
sl_t$trend
sl_t$trendError
```
NOAA reports a value of 2.40 +/- 0.23 mm/year.

#  A Generalized Linear Model Analysis
```{r slr_gls}
the_gls <- gls(MSL_mm ~ MidDate, data=prov_meantrend,
               correlation = corAR1(), method = 'ML')
(s <- summary(the_gls))
```
We need to convert units from days to years.  We can find the info we need
either in the tTable component of the model sumamry:
```{r}
s$tTable[2,1] * 365.2422 
s$tTable[2,2] * 365.2422
```
Or directly 
```{r}
coef(the_gls)[2] * 365.2422
sqrt(vcov(the_gls)[2,2]) * 365.2422
```
So, we get 2.41 +/- 0.12.  If NOAA rounds the estimate only to tenths of 
millimeters, our results match.  Otherwise, they are ever so slightly different.


# The `slr_slope()` Function
The SLR_slope function is a convenience function that wraps up the linear model 
just presented and spits out model coefficients and otehr metadata without 
requiring you to build your own model.

{TODO:  Add code to make this print out in a nicer way, whether by rounding or 
by creating an s3 class and overriding the print method.}

```{r}
slope_info <- slr_slope(prov_meantrend, MSL_mm, MidDate)
print(round(slope_info[1:5],4))
```

Setting t_fit = TRUE makes the model run much more slowly, as it is fitting an
autoregressive model based on the time coordinate, rather than just the sequence
of observations.  The resulting fit is (here) not quite identical, but very 
similar.  

`t_fit == TRUE` will handle unevenly spaced observations appropriately, and may 
be preferred if there are many missing values in the source data.  The following code takes about 30 seconds to a minute to run.

```{r cache = TRUE}
system.time({
slope_info <- slr_slope(prov_meantrend, MSL_mm, MidDate, t_fit = TRUE)
print(round(slope_info[1:5],4))
})
```

## Date Time Formats
With `.mode = 'year'`, which is the default, the time coordinate, `.dt`, must 
be either class "Date" or "POSIXct". The last model was run on dates stores as R `Date` objects.  Here we demonstrate a model run on "POSIXct" times.  It
produces results identical to the analysis based on Date objects.
```{r error = TRUE}
tmp <- prov_meantrend %>%
  mutate(MidDate = as.POSIXct(MidDate))

# this works:
slope_info_raw_POSIX <- slr_slope(tmp, MSL_mm, MidDate, t_fit = FALSE)
round(slope_info_raw_POSIX[1:5],4)
```

But with "POSIXt" objects, trying to fit the model with a covariance structure
based on the time coordinate (in seconds) fails.  This is probably because of
the way the `corAR1()` function is implemented internally.  
```{r error = TRUE}
slope_info_raw_POSIX <- slr_slope(tmp, MSL_mm, MidDate, t_fit = TRUE)
```

##  The `.mode` Argument
If you set `.mode = 'unscaled', the function will report the trend without
rescaling to annual rates.  The rate and its standard error will be reported in
the same units as used to express `.dt`.

You could, for example, create a new variable that starts at zero in January of 
1900, and increments by one each month. Using `.mode = 'unscaled'` produces an
estimate of the long term average monthly rate of change in sea level.  Scaling
that result to an annual basis by hand shows that the result is equivalent
(although not quite identical) to the result when working with Dates.

```{r}
tmp <- prov_meantrend %>%
  mutate(months = (Year - 1900) * 12 + (Month -1))

(res <- round(slr_slope(tmp, MSL_mm, months, .mode = 'unscaled')[1:5],4))
res[['Estimate']] * 12
```

Setting `t_fit = TRUE` works for unscaled analyses too, and in this case, it
runs somewhat faster, presumably because  `months` has one possible value
per observation (absent missing values), making the process fo constructing
the correlation structure faster.

```{r cache = TRUE}
tmp <- prov_meantrend %>%
  mutate(months = (Year - 1900) * 12 + (Month -1))

system.time(res <- round(slr_slope(tmp, MSL_mm, months, .mode = 'unscaled', 
                        t_fit = TRUE)[1:5],4))
res
res[['Estimate']] * 12
```

## Comparison to Results with Data not Seasonally Corrected
NOAA also releases "raw" monthly mean water level data.  Unlike the seasonally
adjusted values used by NOAA to generate sea level rise rate estimates, these 
"raw" water level data are available through the NOAA API.

If we run the same analysis on the "raw" data, we get similar, but again not
identical, results. The most obvious change in  an increase in the estimated
standard error.  That makes sense, as the "seasonally adjusted" data in effect
removes one source of variability from the data. As that variability is still
present, standard errors are a bit higher.
```{r}
slope_info_raw <- slr_slope(prov_monthly, MSL_mm, MidDate, t_fit = FALSE)
print(round(slope_info_raw[1:5],4))
```

# Has Sea Level Rise Increased Recently?
A frequent question raised in analysis of sea level records is whether the rate
of sea level rise is increasing, as predicted by numerous climate models over
the years. This is a simple question that is rather more complex to answer
statistically than it at first appears.

The simplest approach we take to checking this idea is to fit a piece-wise
linear model to historic water level rise data.  The user can specify a 
"breakpoint", "cutpoint" or "knot" by giving a parameter that specifies what 
portion of the data is to be considered the "recent" portion of the data.

This analysis is embodied in the function `slr_change()`.

The piecewise linear model fits the data with two linear segments that join at a
knot.  The easiest way to specify a knot is by the number of years in the
"recent" period.

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, .span = 20, .mode = 'year')
round(s$summary,4)
round(s$details,4)
s$settings

```

## Alternative Time Coordinates
The location of the knot can also be specified by a time interval (of class
"difftime") by specifying `.mode = 'time'`.  It is inconvenient that the 
"difftime" class does not accept time expressed in months, which is often
going to be the units of interest.  For longer time records (over a few years),
one can calculate the (approximate) number of days based on a number of
intervening months by multiplying the number of months by the average number of 
days in a month, 
$$ Y \text{ Days} \approx X \text{ Months} \times 
\frac{365.2422 \text{ Days per Year}}{12 \text{ Months per Year}}$$


We demonstrate by specifying time in months -- here 240 month , which is 
equivalent to  20 years, as before.  We do not expect results to be identical, 
as the "recent" 240 months will line up with the last observation in the data,
not with calendar years.

```{r}
max(prov_meantrend$MidDate)
(our_span <- round(as.difftime(240 * (365.2422/12), units = 'days')))
```

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, 
                .span = our_span, .mode = 'time')
round(s$summary,4)
round(s$details,4)
s$settings

```

Note that the slopes presented here are untransformed, so expressed in units
of days, and need to be converted to units of interest.

Last, the recent period can be defiend by the number of observations in the
recent period by passing an integer and specifying `.mode = 'count'`.

```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, 
                .span = 240, .mode = 'count')
round(s$summary,4)
round(s$details,4)
s$settings

```
Note that the slopes presented here are untransformed, so expressed in units
of days, and need to be converted to units of interest.


## Examining Model Results
It is possible, and sometimes helpful, to retain the model results for further
examination or processing. Because R model objects encapsulate the source data, 
they are often quite large, so `we`slr_change()` does not return the fitted 
model object unless specifically requested.

It is often worth examining the models to make sure you understand how the model
was fit, review model diagnostics, etc.

One circumstance in which the model object is useful is if you want to prepare
a graphic showing the piecewise linear model.

First, we re-run the model indicating that we want to fitted model object.
We then and extract the model component, and draw a graphic.
```{r}
s <- slr_change(prov_meantrend, MSL_mm, MidDate, .span = 20, 
            .mode = 'year', retain_model = TRUE)
```
```{r}
piecewise_gls <- s$model
```

```{r}
summary(piecewise_gls)
```

Notice that the model parameters are untransformed, and thus expressed in 
units per day, not per year.  hi sis likely to be especially disconcerting if 
originl data were a POSIXct object, and thus expressed in seconds. Under those 
conditions, and with the default rounding for the display of model parameters,
it is likely the slopes will appear to be exactly zero.

TODO: Consider changing code so that rescaling occurs before model fitting
rather than after model fitting, to avoid this problem

## Visualizing the Models
```{r  visualize_models, plot_slr_models, fig.height = 7, fig.width = 5}
ggplot(prov_meantrend, aes(MidDate, MSL_mm, group = before_gap)) +
  geom_line(color='grey20', alpha = 0.25 ) +
  geom_line(aes(x = MidDate, y = predict(the_gls)),
            color = 'black', lwd = 1) +
  geom_line(aes(x = MidDate, y = predict(piecewise_gls)),
            color = 'green4', lwd = 1) +
  xlab('Date') + 
  ylab('Monthly Mean Tide Level (m, MSL)')
```

# Is that most recent slope really exceptional?


